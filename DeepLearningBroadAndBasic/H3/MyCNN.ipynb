{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/apps/bypy ($t $f $s $m $d):\n",
      "F food-11.zip 1163028761 2021-01-22, 20:26:42 7d9bbaedcqce1982041e54fba1f8c89f\n",
      "[====================] 100% (1.1GB/1.1GB) ETA:  (7MB/s, 2m50s gone)   e)  "
     ]
    }
   ],
   "source": [
    "!bypy list\n",
    "!bypy downfile food-11.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件已解压\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "### 解压原始数据集，将src_path路径下的zip包解压至data/dataset目录下\n",
    "src_path=\"/home/mist/food-11.zip\"\n",
    "target_path=\"/home/mist/dataset\"\n",
    "def unzip_data(src_path,target_path):\n",
    "    if(not os.path.isdir(target_path)):    \n",
    "        z = zipfile.ZipFile(src_path, 'r')    # 只读方式打开压缩文件\n",
    "        z.extractall(path=target_path)        # 提取其中内容\n",
    "        z.close()\n",
    "    else:\n",
    "        print(\"文件已解压\")\n",
    "unzip_data(src_path,target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import必要的第三方库\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfile(path, label):\n",
    "    # label 是一个 boolean variable，代表需不需要回传 y 值\n",
    "    image_dir = sorted(os.listdir(path))\n",
    "    x = np.zeros((len(image_dir), 128, 128, 3), dtype=np.uint8)\n",
    "    y = np.zeros((len(image_dir)), dtype=np.uint8)\n",
    "    for i, file in enumerate(image_dir):\n",
    "        img = cv2.imread(os.path.join(path, file))\n",
    "        x[i, :, :] = cv2.resize(img,(128, 128))\n",
    "        if label:\n",
    "          y[i] = int(file.split(\"_\")[0])\n",
    "    if label:\n",
    "      return x, y\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data\n",
      "Size of training data = 9866\n",
      "Size of validation data = 3430\n",
      "Size of Testing data = 3347\n"
     ]
    }
   ],
   "source": [
    "#分別计算training set、validation set、testing set 的大小\n",
    "workspace_dir = './dataset/food-11'\n",
    "print(\"Reading data\")\n",
    "train_x, train_y = readfile(os.path.join(workspace_dir, \"training\"), True)\n",
    "print(\"Size of training data = {}\".format(len(train_x)))\n",
    "val_x, val_y = readfile(os.path.join(workspace_dir, \"validation\"), True)\n",
    "print(\"Size of validation data = {}\".format(len(val_x)))\n",
    "test_x = readfile(os.path.join(workspace_dir, \"testing\"), False)\n",
    "print(\"Size of Testing data = {}\".format(len(test_x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "### 在 Pytorch 中，我们可以利用 torch.utils.data 的 Dataset 及 DataLoader 来\"包装\" data，使后续的 training 及 testing 更为方便。\n",
    "\n",
    "### Dataset 需要 overload 两个函数：__len__ 及 __getitem__\n",
    "\n",
    "### __len__ 必须要回传 dataset 的大小，而 __getitem__ 则定义了当程式利用 取值时，dataset 应该要怎么回传资料。\n",
    "\n",
    "### 实际上我们并不会直接使用到这两个函数，但是使用 DataLoader 在 enumerate Dataset 时会使用到，没有实做的话会在程式运行阶段出现 error。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomHorizontalFlip(), #随机将图片水平翻转\n",
    "    transforms.RandomRotation(15), #随机旋转图片\n",
    "    transforms.ToTensor(), #将图片转成 Tensor\n",
    "])\n",
    "#testing 时不需做 data augmentation\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),                                    \n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "class ImgDataset(Dataset):\n",
    "    def __init__(self, x, y=None, transform=None):\n",
    "        self.x = x\n",
    "        # label is required to be a LongTensor\n",
    "        self.y = y\n",
    "        if y is not None:\n",
    "            self.y = torch.LongTensor(y)\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, index):\n",
    "        X = self.x[index]\n",
    "        if self.transform is not None:\n",
    "            X = self.transform(X)\n",
    "        if self.y is not None:\n",
    "            Y = self.y[index]\n",
    "            return X, Y\n",
    "        else:\n",
    "            return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_set = ImgDataset(train_x, train_y, train_transform)\n",
    "val_set = ImgDataset(val_x, val_y, test_transform)\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        #torch.nn.MaxPool2d(kernel_size, stride, padding)\n",
    "        #input 維度 [3, 128, 128]\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, 1, 1),  # [64, 128, 128]\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),      # [64, 64, 64]\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, 1, 1), # [128, 64, 64]\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),      # [128, 32, 32]\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, 1, 1), # [256, 32, 32]\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),      # [256, 16, 16]\n",
    "\n",
    "            nn.Conv2d(256, 512, 3, 1, 1), # [512, 16, 16]\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),       # [512, 8, 8]\n",
    "            \n",
    "            nn.Conv2d(512, 512, 3, 1, 1), # [512, 8, 8]\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),       # [512, 4, 4]\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512*4*4, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 11)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)\n",
    "        out = out.view(out.size()[0], -1)\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/030] 28.74 sec(s) Train Acc: 0.248125 Loss: 0.017763 | Val Acc: 0.260641 loss: 0.017595\n",
      "[002/030] 28.40 sec(s) Train Acc: 0.345834 Loss: 0.014737 | Val Acc: 0.287755 loss: 0.015745\n",
      "[003/030] 28.36 sec(s) Train Acc: 0.398236 Loss: 0.013635 | Val Acc: 0.422449 loss: 0.013225\n",
      "[004/030] 29.00 sec(s) Train Acc: 0.446888 Loss: 0.012517 | Val Acc: 0.388047 loss: 0.014008\n",
      "[005/030] 28.60 sec(s) Train Acc: 0.472025 Loss: 0.011908 | Val Acc: 0.439942 loss: 0.013375\n",
      "[006/030] 28.33 sec(s) Train Acc: 0.513785 Loss: 0.011115 | Val Acc: 0.472303 loss: 0.012677\n",
      "[007/030] 28.16 sec(s) Train Acc: 0.533651 Loss: 0.010510 | Val Acc: 0.160350 loss: 0.032364\n",
      "[008/030] 28.18 sec(s) Train Acc: 0.564667 Loss: 0.009979 | Val Acc: 0.501458 loss: 0.011824\n",
      "[009/030] 28.35 sec(s) Train Acc: 0.581593 Loss: 0.009466 | Val Acc: 0.529155 loss: 0.011155\n",
      "[010/030] 28.30 sec(s) Train Acc: 0.609872 Loss: 0.008984 | Val Acc: 0.560641 loss: 0.010725\n",
      "[011/030] 28.20 sec(s) Train Acc: 0.622441 Loss: 0.008601 | Val Acc: 0.552478 loss: 0.010555\n",
      "[012/030] 28.63 sec(s) Train Acc: 0.634097 Loss: 0.008213 | Val Acc: 0.568513 loss: 0.010808\n",
      "[013/030] 28.36 sec(s) Train Acc: 0.667748 Loss: 0.007616 | Val Acc: 0.512828 loss: 0.012040\n",
      "[014/030] 28.40 sec(s) Train Acc: 0.666025 Loss: 0.007700 | Val Acc: 0.607580 loss: 0.009150\n",
      "[015/030] 28.27 sec(s) Train Acc: 0.687310 Loss: 0.007077 | Val Acc: 0.578134 loss: 0.010290\n",
      "[016/030] 28.28 sec(s) Train Acc: 0.707582 Loss: 0.006643 | Val Acc: 0.616035 loss: 0.009339\n",
      "[017/030] 28.30 sec(s) Train Acc: 0.724103 Loss: 0.006212 | Val Acc: 0.587464 loss: 0.011186\n",
      "[018/030] 28.31 sec(s) Train Acc: 0.734036 Loss: 0.006103 | Val Acc: 0.553936 loss: 0.012556\n",
      "[019/030] 28.43 sec(s) Train Acc: 0.750862 Loss: 0.005622 | Val Acc: 0.589213 loss: 0.011070\n",
      "[020/030] 29.83 sec(s) Train Acc: 0.757855 Loss: 0.005546 | Val Acc: 0.610787 loss: 0.009899\n",
      "[021/030] 28.48 sec(s) Train Acc: 0.767991 Loss: 0.005305 | Val Acc: 0.603207 loss: 0.010690\n",
      "[022/030] 28.41 sec(s) Train Acc: 0.771640 Loss: 0.005106 | Val Acc: 0.601749 loss: 0.010961\n",
      "[023/030] 28.42 sec(s) Train Acc: 0.787452 Loss: 0.004583 | Val Acc: 0.620991 loss: 0.010324\n",
      "[024/030] 28.45 sec(s) Train Acc: 0.816035 Loss: 0.004141 | Val Acc: 0.587755 loss: 0.011960\n",
      "[025/030] 28.64 sec(s) Train Acc: 0.822623 Loss: 0.004037 | Val Acc: 0.603499 loss: 0.011448\n",
      "[026/030] 29.52 sec(s) Train Acc: 0.812994 Loss: 0.004240 | Val Acc: 0.625656 loss: 0.010211\n",
      "[027/030] 28.33 sec(s) Train Acc: 0.821508 Loss: 0.004020 | Val Acc: 0.646647 loss: 0.010013\n",
      "[028/030] 28.24 sec(s) Train Acc: 0.844517 Loss: 0.003487 | Val Acc: 0.625948 loss: 0.010817\n",
      "[029/030] 28.21 sec(s) Train Acc: 0.846544 Loss: 0.003465 | Val Acc: 0.655685 loss: 0.010114\n",
      "[030/030] 28.31 sec(s) Train Acc: 0.867018 Loss: 0.003090 | Val Acc: 0.626239 loss: 0.011587\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "model = Classifier().cuda()\n",
    "loss = nn.CrossEntropyLoss() # 因为是 classification task，所以 loss 使用 CrossEntropyLoss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # optimizer 使用 Adam\n",
    "num_epoch = 30 # 训练轮数\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    epoch_start_time = time.time()\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    val_loss = 0.0\n",
    "\n",
    "    model.train() # 确保 model 是在 train model (开启 Dropout 等...)\n",
    "    for i, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad() # 用 optimizer 将 model 参数的 gradient 归零\n",
    "        train_pred = model(data[0].cuda()) # 利用 model 得到预测的机率分布 这边实际上就是去呼叫 model 的 forward 函数\n",
    "        batch_loss = loss(train_pred, data[1].cuda()) # 计算 loss （注意 prediction 跟 label 必须同时在 CPU 或是 GPU 上）\n",
    "        batch_loss.backward() # 利用 back propagation 算出每个参数的 gradient\n",
    "        optimizer.step() # 以 optimizer 用 gradient 更新参数值\n",
    "\n",
    "        train_acc += np.sum(np.argmax(train_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
    "        train_loss += batch_loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader):\n",
    "            val_pred = model(data[0].cuda())\n",
    "            batch_loss = loss(val_pred, data[1].cuda())\n",
    "\n",
    "            val_acc += np.sum(np.argmax(val_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
    "            val_loss += batch_loss.item()\n",
    "\n",
    "        #將結果 print 出來\n",
    "        print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \\\n",
    "            (epoch + 1, num_epoch, time.time()-epoch_start_time, \\\n",
    "             train_acc/train_set.__len__(), train_loss/train_set.__len__(), val_acc/val_set.__len__(), val_loss/val_set.__len__()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 得到好的参数后，我们使用training set和validation set共同训练（资料量变多，模型效果较好）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_x = np.concatenate((train_x, val_x), axis=0)\n",
    "train_val_y = np.concatenate((train_y, val_y), axis=0)\n",
    "train_val_set = ImgDataset(train_val_x, train_val_y, train_transform)\n",
    "train_val_loader = DataLoader(train_val_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/030] 34.48 sec(s) Train Acc: 0.256919 Loss: 0.017188\n",
      "[002/030] 34.64 sec(s) Train Acc: 0.369434 Loss: 0.014016\n",
      "[003/030] 34.26 sec(s) Train Acc: 0.450963 Loss: 0.012416\n",
      "[004/030] 34.08 sec(s) Train Acc: 0.499097 Loss: 0.011280\n",
      "[005/030] 34.32 sec(s) Train Acc: 0.543998 Loss: 0.010251\n",
      "[006/030] 34.49 sec(s) Train Acc: 0.576038 Loss: 0.009489\n",
      "[007/030] 34.32 sec(s) Train Acc: 0.609431 Loss: 0.008827\n",
      "[008/030] 34.19 sec(s) Train Acc: 0.644780 Loss: 0.008078\n",
      "[009/030] 34.52 sec(s) Train Acc: 0.667494 Loss: 0.007496\n",
      "[010/030] 34.31 sec(s) Train Acc: 0.682085 Loss: 0.007200\n",
      "[011/030] 34.34 sec(s) Train Acc: 0.700361 Loss: 0.006714\n",
      "[012/030] 34.11 sec(s) Train Acc: 0.726008 Loss: 0.006303\n",
      "[013/030] 34.35 sec(s) Train Acc: 0.742103 Loss: 0.005884\n",
      "[014/030] 34.40 sec(s) Train Acc: 0.748797 Loss: 0.005606\n",
      "[015/030] 34.76 sec(s) Train Acc: 0.773691 Loss: 0.005144\n",
      "[016/030] 34.28 sec(s) Train Acc: 0.781965 Loss: 0.004921\n",
      "[017/030] 34.06 sec(s) Train Acc: 0.796255 Loss: 0.004533\n",
      "[018/030] 34.12 sec(s) Train Acc: 0.812274 Loss: 0.004194\n",
      "[019/030] 34.05 sec(s) Train Acc: 0.821676 Loss: 0.003989\n",
      "[020/030] 34.11 sec(s) Train Acc: 0.831528 Loss: 0.003823\n",
      "[021/030] 35.06 sec(s) Train Acc: 0.846119 Loss: 0.003490\n",
      "[022/030] 34.63 sec(s) Train Acc: 0.851835 Loss: 0.003276\n",
      "[023/030] 34.38 sec(s) Train Acc: 0.869510 Loss: 0.002909\n",
      "[024/030] 34.33 sec(s) Train Acc: 0.881543 Loss: 0.002647\n",
      "[025/030] 34.33 sec(s) Train Acc: 0.889967 Loss: 0.002490\n",
      "[026/030] 34.61 sec(s) Train Acc: 0.894254 Loss: 0.002415\n",
      "[027/030] 34.47 sec(s) Train Acc: 0.896209 Loss: 0.002298\n",
      "[028/030] 34.43 sec(s) Train Acc: 0.910800 Loss: 0.001980\n",
      "[029/030] 34.09 sec(s) Train Acc: 0.916291 Loss: 0.001899\n",
      "[030/030] 34.22 sec(s) Train Acc: 0.925015 Loss: 0.001693\n"
     ]
    }
   ],
   "source": [
    "model_best = Classifier().cuda()\n",
    "loss = nn.CrossEntropyLoss() # 因为是 classification task，所以 loss 使用 CrossEntropyLoss\n",
    "optimizer = torch.optim.Adam(model_best.parameters(), lr=0.001) # optimizer 使用 Adam\n",
    "num_epoch = 30\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    epoch_start_time = time.time()\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "\n",
    "    model_best.train()\n",
    "    for i, data in enumerate(train_val_loader):\n",
    "        optimizer.zero_grad()\n",
    "        train_pred = model_best(data[0].cuda())\n",
    "        batch_loss = loss(train_pred, data[1].cuda())\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_acc += np.sum(np.argmax(train_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
    "        train_loss += batch_loss.item()\n",
    "\n",
    "        #将結果 print 出來\n",
    "    print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f' % \\\n",
    "      (epoch + 1, num_epoch, time.time()-epoch_start_time, \\\n",
    "      train_acc/train_val_set.__len__(), train_loss/train_val_set.__len__()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "### 利用刚刚 train 好的 model 进行 prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = ImgDataset(test_x, transform=test_transform)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_best.eval()\n",
    "prediction = []\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader):\n",
    "        test_pred = model_best(data.cuda())\n",
    "        test_label = np.argmax(test_pred.cpu().data.numpy(), axis=1)\n",
    "        for y in test_label:\n",
    "            prediction.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将结果写入 csv 档\n",
    "with open(\"predict.csv\", 'w') as f:\n",
    "    f.write('Id,Category\\n')\n",
    "    for i, y in  enumerate(prediction):\n",
    "        f.write('{},{}\\n'.format(i, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state,filename=\"checkpoint.pth\"):\n",
    "    print(\"Saving checkpoint\")\n",
    "    torch.save(state,filename)\n",
    "checkpoint={'state_dict':model_best.state_dict(),'optimizer':optimizer.state_dict()}\n",
    "save_checkpoint(checkpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
